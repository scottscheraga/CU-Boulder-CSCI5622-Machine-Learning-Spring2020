{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semester Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "#NOTE!\n",
    "#Please run the following in terminal before running this\n",
    "\n",
    "pip3 install -U yellowbrick\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "# Import TfidfVectorizer to create TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Import cosine_similarity to calculate similarity of movie plot\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#https://www.scikit-yb.org/en/latest/api/cluster/elbow.html\n",
    "from sklearn.datasets import make_blobs\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "%matplotlib inline\n",
    "# Import modules necessary to plot dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "\n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [y for x in nltk.sent_tokenize(text) for y in nltk.word_tokenize(x)]\n",
    "\n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "\n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "    return stems\n",
    "\"\"\"\n",
    "#stop2 =  ['of ','is ','to ','in ','and ','the ']\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "#Scott's version\n",
    "def tokenize_and_stem(text):\n",
    "    sentence=text\n",
    "    \n",
    "    #sentence = sentence.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop2)]))\n",
    "    #sentence = ' '.join([word for word in text.split() if word not in stop2])\n",
    "    \n",
    "    sentence=re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence=sentence.lower()  #Make all words lowercase\n",
    "    sentence=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)   #remove tags\n",
    "    sentence=re.sub(\"(\\\\d|\\\\W)+\",\" \",sentence) # remove special characters and digits\n",
    "    #tokens = [y for x in nltk.sent_tokenize(sentence) for y in nltk.word_tokenize(x)]\n",
    "    \n",
    "   # for i in stop2 :\n",
    "     #   sentence=re.sub(i, '', sentence)\n",
    "        \n",
    "    templist= sentence.split()\n",
    "\n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    templist = [token for token in templist if re.search('[a-zA-Z]', token)]\n",
    "    #stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    lem = WordNetLemmatizer()#Lemmatisation\n",
    "    templist = [lem.lemmatize(word) for word in templist if not word in  \n",
    "                stop_words]\n",
    "    \n",
    "    stems = [stemmer.stem(word) for word in templist]  \n",
    "   # test['tweet_without_stopwords'] = test['tweet'].apply(lambda x: ' '.join(\n",
    "        #            [word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    return stems\n",
    "\n",
    "\"\"\"\n",
    "#http://brandonrose.org/clustering\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "# parameters for efficient processing of text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=20000,\n",
    "                                 min_df=0.12, stop_words='english',tokenizer=tokenize_and_stem,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,3))\n",
    "tfidf_vectorizer0 = TfidfVectorizer(max_df=0.8, max_features=20000,min_df=0.12, stop_words='english',tokenizer=tokenize_and_stem, use_idf=True,ngram_range=(1,3))\n",
    "tfidf_vectorizer1 = TfidfVectorizer(max_df=0.8, max_features=20000,min_df=0.12, stop_words='english',tokenizer=tokenize_and_stem, use_idf=True,ngram_range=(1,3))\n",
    "tfidf_vectorizer2 = TfidfVectorizer(max_df=0.8, max_features=20000,min_df=0.12, stop_words='english', tokenizer=tokenize_and_stem,use_idf=True,ngram_range=(1,3))\n",
    "tfidf_vectorizer3 = TfidfVectorizer(max_df=0.8, max_features=20000,min_df=0.12, stop_words='english',tokenizer=tokenize_and_stem, use_idf=True,ngram_range=(1,3))\n",
    "tfidf_vectorizer4 = TfidfVectorizer(max_df=0.8, max_features=20000,min_df=0.12, stop_words='english',tokenizer=tokenize_and_stem, use_idf=True,ngram_range=(1,3))\n",
    "tfidf_vectorizer5 = TfidfVectorizer(max_df=0.8, max_features=20000,min_df=0.12, stop_words='english',tokenizer=tokenize_and_stem, use_idf=True,ngram_range=(1,3))\n",
    "tfidf_vectorizer6 = TfidfVectorizer(max_df=0.8, max_features=20000,min_df=0.12, stop_words='english',tokenizer=tokenize_and_stem, use_idf=True,ngram_range=(1,3))\n",
    "\n",
    "\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer0=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer1=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer2=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer3=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer4=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer5=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer6=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "\n",
    "\n",
    "tfidf_vectorizer_overview = TfidfVectorizer(max_df=0.8, max_features=20000,\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True,tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))\n",
    "\n",
    "\n",
    "\n",
    "words_stemmed = tokenize_and_stem(\"Today (May 19, 2016) of to in is his only daughter's wedding.\")\n",
    "print(words_stemmed)\n",
    "\n",
    "#, tokenizer=tokenize_and_stem\n",
    "#original max features=200000\n",
    "# tokenizer=tokenize_and_stem,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def elbow_analysis(self, k_range):\n",
    "        '''\n",
    "        PARAMS:\n",
    "            k_range : (list of integers) contains the K number of\n",
    "            hyperparameters k to peform the analysis over\n",
    "        RETURN:\n",
    "            avg_variance : (list of float) list of size K. contains the average variance \n",
    "            of clusters corresponding to each to each hyperparameter k\n",
    "        '''\n",
    "        a,b = self.X.shape\n",
    "        avg_variance=[]\n",
    "        for k in k_range:\n",
    "                #print(\"k=\",k)\n",
    "                self.K=k\n",
    "                self.centroids=self.initialize_centroids()\n",
    "                \n",
    "                self.cluster(20,False)\n",
    "                \n",
    "                mindist=sum(np.min(self.compute_distances(),axis=1))\n",
    "                #print(\"mindist=\",mindist)\n",
    "                #print(\"a=\",a)\n",
    "                avgmindist=mindist/a\n",
    "                avg_variance.append(avgmindist)\n",
    "                #print(avg_variance)\n",
    "        \n",
    "        return avg_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://shravan-kuchkula.github.io/clustering_movie_plots/#2-combine-wikipedia-and-imdb-plot-summaries\n",
    "\"\"\"\n",
    "\n",
    "#a.drop(indexNames , inplace=True)\n",
    "\"\"\"\n",
    "indexNames2 = a[ a['country'] != \"USA\" ].index \n",
    "a.drop(indexNames2 , inplace=True)\n",
    "indexNames3 = a[ a['year'] != 2016 ].index\n",
    "a.drop(indexNames3 , inplace=True)\n",
    "print(a.info())\n",
    "#print(a.groupby(\"country\").mean())  \n",
    "#print(a['country'].value_counts())\n",
    "#print(max(a['year']))\n",
    "\n",
    "\"\"\"\n",
    "TMDB= pd.read_csv(\"TMDBalldatafrom2019to1999.csv\",delimiter='|' ) \n",
    "#Movies from 2000 to 2019\n",
    "#english ONLY\n",
    "#print(TMDB.info())\n",
    "TMDB['profit'] = TMDB['revenue'] - TMDB['budget']\n",
    "#indexNames3 = TMDB[TMDB['year'] != 2016 ].index\n",
    "#TMDB.drop(indexNames3 , inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "TMDB.dropna(subset = [\"overview\"], inplace=True)\n",
    "TMDB.dropna(subset = [\"genres\"], inplace=True)\n",
    "\n",
    "#https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['the','life','lives','one','young','when','them','they']\n",
    "stopwords.extend(newStopWords)\n",
    "\n",
    "\n",
    "TMDB['overview2'] = TMDB['overview'].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "print(TMDB.info())\n",
    "#print(TMDB.info())\n",
    "#pd.options.display.max_rows = 4000\n",
    "#print(TMDB['genres'].value_counts())\n",
    "\n",
    "#TMDB.head()  #displays first rows of data nicely\n",
    "\n",
    "#https://stackoverflow.com/questions/34449127/sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "#terms = tfidf_vectorizer.get_feature_names()\n",
    "#df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names())\n",
    "#print(df)\n",
    "\n",
    "tfidf_matrix =  tfidf_vectorizer.fit_transform([x for x in TMDB[\"genres\"]])\n",
    "\n",
    "\n",
    "#https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/\n",
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform([x for x in TMDB[\"genres\"]])\n",
    "tfidf_valuelist= tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_valuelist.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "\n",
    "\n",
    "print(\"idf_weights for all film genres\")\n",
    "print(df_idf.sort_values(by=['idf_weights']))# sort ascending\n",
    "\n",
    "\n",
    "#print(tfidf_matrix.shape)\n",
    "#print(tfidf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(4,12))\n",
    "\n",
    "visualizer.fit(tfidf_matrix)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "GenreClustertotal=7\n",
    "km = KMeans(n_clusters=GenreClustertotal)\n",
    "# Fit the k-means object with tfidf_matrix\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "# Create a column cluster to denote the generated cluster for each movie\n",
    "TMDB[\"genre_cluster\"] = clusters\n",
    "\n",
    "# Display number of films per cluster (clusters from 0 to 4)\n",
    "print(\"genre_clusters\",TMDB['genre_cluster'].value_counts())\n",
    "#--------------------------------------------\n",
    "\n",
    "\n",
    "TMDBgenrecluster0=TMDB[TMDB['genre_cluster'] == 0]\n",
    "TMDBgenrecluster1=TMDB[TMDB['genre_cluster'] == 1]\n",
    "TMDBgenrecluster2=TMDB[TMDB['genre_cluster'] == 2]\n",
    "TMDBgenrecluster3=TMDB[TMDB['genre_cluster'] == 3]\n",
    "TMDBgenrecluster4=TMDB[TMDB['genre_cluster'] == 4]\n",
    "TMDBgenrecluster5=TMDB[TMDB['genre_cluster'] == 5]\n",
    "TMDBgenrecluster6=TMDB[TMDB['genre_cluster'] == 6]\n",
    "\n",
    "tfidf_matrix0 =  tfidf_vectorizer0.fit_transform([x for x in TMDBgenrecluster0[\"genres\"]])\n",
    "\n",
    "tfidf_matrix1 =  tfidf_vectorizer1.fit_transform([x for x in TMDBgenrecluster1[\"genres\"]])\n",
    "tfidf_matrix2 =  tfidf_vectorizer2.fit_transform([x for x in TMDBgenrecluster2[\"genres\"]])\n",
    "tfidf_matrix3 =  tfidf_vectorizer3.fit_transform([x for x in TMDBgenrecluster3[\"genres\"]])\n",
    "tfidf_matrix4 =  tfidf_vectorizer4.fit_transform([x for x in TMDBgenrecluster4[\"genres\"]])\n",
    "tfidf_matrix5 =  tfidf_vectorizer5.fit_transform([x for x in TMDBgenrecluster5[\"genres\"]])\n",
    "tfidf_matrix6 =  tfidf_vectorizer6.fit_transform([x for x in TMDBgenrecluster6[\"genres\"]])\n",
    "\n",
    "cv0=CountVectorizer()\n",
    "cv1=CountVectorizer()\n",
    "cv2=CountVectorizer()\n",
    "cv3=CountVectorizer()\n",
    "cv4=CountVectorizer()\n",
    "cv5=CountVectorizer()\n",
    "cv6=CountVectorizer()\n",
    "\n",
    "word_count_vector0=cv0.fit_transform([a for a in TMDBgenrecluster0[\"genres\"]])\n",
    "print(word_count_vector0.shape)\n",
    "word_count_vector1=cv1.fit_transform([b for b in TMDBgenrecluster1[\"genres\"]])\n",
    "word_count_vector2=cv2.fit_transform([c for c in TMDBgenrecluster2[\"genres\"]])\n",
    "word_count_vector3=cv3.fit_transform([d for d in TMDBgenrecluster3[\"genres\"]])\n",
    "word_count_vector4=cv4.fit_transform([e for e in TMDBgenrecluster4[\"genres\"]])\n",
    "word_count_vector5=cv5.fit_transform([f for f in TMDBgenrecluster5[\"genres\"]])\n",
    "word_count_vector6=cv6.fit_transform([g for g in TMDBgenrecluster6[\"genres\"]])\n",
    "print(word_count_vector6.shape)\n",
    "\n",
    "tfidf_valuelist0= tfidf_transformer0.fit(word_count_vector0)\n",
    "tfidf_valuelist1= tfidf_transformer1.fit(word_count_vector1)\n",
    "tfidf_valuelist2= tfidf_transformer2.fit(word_count_vector2)\n",
    "tfidf_valuelist3= tfidf_transformer3.fit(word_count_vector3)\n",
    "tfidf_valuelist4= tfidf_transformer4.fit(word_count_vector4)\n",
    "tfidf_valuelist5= tfidf_transformer5.fit(word_count_vector5)\n",
    "tfidf_valuelist6= tfidf_transformer6.fit(word_count_vector6)\n",
    "\n",
    "\n",
    "df_idf0 = pd.DataFrame(tfidf_valuelist0.idf_, index=cv0.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf1 = pd.DataFrame(tfidf_valuelist1.idf_, index=cv1.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf2 = pd.DataFrame(tfidf_valuelist2.idf_, index=cv2.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf3 = pd.DataFrame(tfidf_valuelist3.idf_, index=cv3.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf4 = pd.DataFrame(tfidf_valuelist4.idf_, index=cv4.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf5 = pd.DataFrame(tfidf_valuelist5.idf_, index=cv5.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf6 = pd.DataFrame(tfidf_valuelist6.idf_, index=cv6.get_feature_names(),columns=[\"idf_weights\"])\n",
    "\n",
    "\n",
    "print(\"cluster 0\")\n",
    "print(df_idf0.sort_values(by=['idf_weights']))\n",
    "print(\"cluster 1\")\n",
    "print(df_idf1.sort_values(by=['idf_weights']))\n",
    "print(\"cluster 2\")\n",
    "print(df_idf2.sort_values(by=['idf_weights']))\n",
    "print(\"cluster 3\")\n",
    "print(df_idf3.sort_values(by=['idf_weights']))\n",
    "print(\"cluster 4\")\n",
    "print(df_idf4.sort_values(by=['idf_weights']))\n",
    "print(\"cluster 5\")\n",
    "print(df_idf5.sort_values(by=['idf_weights']))\n",
    "print(\"cluster 6\")\n",
    "print(df_idf6.sort_values(by=['idf_weights']))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the similarity distance\n",
    "similarity_distance = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "\n",
    "# Create mergings matrix\n",
    "mergings = linkage(similarity_distance, method='complete')\n",
    "\n",
    "# Plot the dendrogram, using title as label column\n",
    "dendrogram_ = dendrogram(mergings, orientation=\"left\",\n",
    "               labels=[x for x in TMDB[\"title\"]],\n",
    "               #leaf_rotation=90,\n",
    "               leaf_font_size=27)\n",
    "\n",
    "# Adjust the plot\n",
    "fig = plt.gcf()\n",
    "_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\n",
    "fig.set_size_inches(80, 80)\n",
    "\n",
    "# Show the plotted dendrogram\n",
    "#plt.savefig('plt.png', format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "#print(TMDB)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "#TMDB.head()\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#subclusterCount0=0\n",
    "subclustertotal=7\n",
    "for a in range(subclustertotal):\n",
    "    #print(\"-----------A=\", a)\n",
    "    indexMain=TMDB[TMDB['genre_cluster'] == a ].index\n",
    "#TMDB.drop(indexNames3 , inplace=True)  \n",
    "    if a==0:\n",
    "        TMDBcluster0=TMDB[TMDB['genre_cluster'] == a ]\n",
    "        tfidf_matrix_overview =  tfidf_vectorizer_overview.fit_transform(\n",
    "                         [x for x in TMDBcluster0[\"overview2\"]]) \n",
    "        \n",
    "        km_overview = KMeans(n_clusters=subclustertotal)\n",
    "        # Fit the k-means object with tfidf_matrix\n",
    "        km_overview.fit(tfidf_matrix_overview)\n",
    "        clusters_overview = km_overview.labels_.tolist()\n",
    "        # Create a column cluster to denote the generated cluster for each movie\n",
    "        TMDBcluster0[\"overview_cluster\"] = clusters_overview\n",
    "        subclusterCount0=TMDBcluster0[\"overview_cluster\"].max()\n",
    "        print(\"subclusterCount0=\",subclusterCount0)\n",
    "        \n",
    "        TMDBcluster0[\"generalcluster\"]=clusters_overview\n",
    "        # Display number of films per cluster (clusters from 0 to 4)\n",
    "        #print(\"overview_cluster\",TMDBcluster0['overview_cluster'].value_counts())\n",
    "        #print(\"generalcluster\",TMDBcluster0['generalcluster'].value_counts())\n",
    "        #print(TMDBcluster0)\n",
    "        \n",
    "        \n",
    "    subclustertotal=7\n",
    "    if a==1:\n",
    "        TMDBcluster1=TMDB[TMDB['genre_cluster'] == a ]\n",
    "        tfidf_matrix_overview =  tfidf_vectorizer_overview.fit_transform(\n",
    "                         [x for x in TMDBcluster1[\"overview2\"]]) \n",
    "        km_overview = KMeans(n_clusters=subclustertotal)\n",
    "        km_overview.fit(tfidf_matrix_overview)\n",
    "        clusters_overview = km_overview.labels_.tolist()\n",
    "        TMDBcluster1[\"overview_cluster\"] = clusters_overview\n",
    "        subclusterCount1=TMDBcluster1[\"overview_cluster\"].max()\n",
    "        print(\"subclusterCount1=\",subclusterCount1)\n",
    "        TMDBcluster1[\"generalcluster\"]=clusters_overview\n",
    "        TMDBcluster1[\"generalcluster\"]+=1+subclusterCount0\n",
    "        #print(\"overview_cluster\",TMDBcluster1['overview_cluster'].value_counts())\n",
    "        #print(\"generalcluster\",TMDBcluster1['generalcluster'].value_counts())\n",
    "    if a==2:\n",
    "        TMDBcluster2=TMDB[TMDB['genre_cluster'] == a ]\n",
    "        tfidf_matrix_overview =  tfidf_vectorizer_overview.fit_transform(\n",
    "                         [x for x in TMDBcluster2[\"overview2\"]]) \n",
    "        km_overview = KMeans(n_clusters=subclustertotal)\n",
    "        km_overview.fit(tfidf_matrix_overview)\n",
    "        clusters_overview = km_overview.labels_.tolist()\n",
    "        TMDBcluster2[\"overview_cluster\"] = clusters_overview\n",
    "        subclusterCount2=TMDBcluster2[\"overview_cluster\"].max()\n",
    "        print(\"subclusterCount2=\",subclusterCount2)        \n",
    "        TMDBcluster2[\"generalcluster\"]=clusters_overview\n",
    "        TMDBcluster2[\"generalcluster\"]+=2+subclusterCount0+subclusterCount1\n",
    "        #print(\"overview_cluster\",TMDBcluster2['overview_cluster'].value_counts())\n",
    "        #print(\"generalcluster\",TMDBcluster2['generalcluster'].value_counts())\n",
    "    if a==3:\n",
    "        TMDBcluster3=TMDB[TMDB['genre_cluster'] == a ]\n",
    "        tfidf_matrix_overview =  tfidf_vectorizer_overview.fit_transform(\n",
    "                         [x for x in TMDBcluster3[\"overview2\"]]) \n",
    "        km_overview = KMeans(n_clusters=subclustertotal)\n",
    "        km_overview.fit(tfidf_matrix_overview)\n",
    "        clusters_overview = km_overview.labels_.tolist()\n",
    "        TMDBcluster3[\"overview_cluster\"] = clusters_overview\n",
    "        subclusterCount3=TMDBcluster3[\"overview_cluster\"].max()\n",
    "        print(\"subclusterCount3=\",subclusterCount3)\n",
    "        #TMDBcluster3[\"generalcluster\"]=(clusters_overview+1+\n",
    "         #  subclusterCount0+subclusterCount1+subclusterCount2)\n",
    "        TMDBcluster3[\"generalcluster\"]=clusters_overview\n",
    "        TMDBcluster3[\"generalcluster\"]+=3+subclusterCount0+subclusterCount1+subclusterCount2\n",
    "        #print(\"overview_cluster\",TMDBcluster3['overview_cluster'].value_counts())\n",
    "        #print(\"generalcluster\",TMDBcluster3['generalcluster'].value_counts())\n",
    "    if a==4:\n",
    "        TMDBcluster4=TMDB[TMDB['genre_cluster'] == a ]\n",
    "        tfidf_matrix_overview =  tfidf_vectorizer_overview.fit_transform(\n",
    "                         [x for x in TMDBcluster4[\"overview2\"]]) \n",
    "        km_overview = KMeans(n_clusters=subclustertotal)\n",
    "        km_overview.fit(tfidf_matrix_overview)\n",
    "        clusters_overview = km_overview.labels_.tolist()\n",
    "        TMDBcluster4[\"overview_cluster\"] = clusters_overview\n",
    "        subclusterCount4=TMDBcluster4[\"overview_cluster\"].max()\n",
    "        print(\"subclusterCount4=\",subclusterCount4)\n",
    "        TMDBcluster4[\"generalcluster\"]=clusters_overview\n",
    "        TMDBcluster4[\"generalcluster\"]+=(4+subclusterCount0+subclusterCount1+subclusterCount2\n",
    "                                            +subclusterCount3)\n",
    "        \n",
    "        #print(\"overview_cluster\",TMDBcluster4['overview_cluster'].value_counts())\n",
    "        #print(\"generalcluster\",TMDBcluster4['generalcluster'].value_counts())\n",
    "    if a==5:\n",
    "        TMDBcluster5=TMDB[TMDB['genre_cluster'] == a ]\n",
    "        tfidf_matrix_overview =  tfidf_vectorizer_overview.fit_transform(\n",
    "                         [x for x in TMDBcluster5[\"overview2\"]]) \n",
    "        km_overview = KMeans(n_clusters=subclustertotal)\n",
    "        km_overview.fit(tfidf_matrix_overview)\n",
    "        clusters_overview = km_overview.labels_.tolist()\n",
    "        TMDBcluster5[\"overview_cluster\"] = clusters_overview\n",
    "        subclusterCount5=TMDBcluster5[\"overview_cluster\"].max()\n",
    "        print(\"subclusterCount5=\",subclusterCount5)\n",
    "        #print(\"modifier= \",(5+subclusterCount0+subclusterCount1+subclusterCount2\n",
    "                           #                 +subclusterCount3+subclusterCount4))\n",
    "        TMDBcluster5[\"generalcluster\"]=clusters_overview\n",
    "        TMDBcluster5[\"generalcluster\"]+=(5+subclusterCount0+subclusterCount1+subclusterCount2\n",
    "                                            +subclusterCount3+subclusterCount4)\n",
    "        \n",
    "        #print(\"overview_cluster\",TMDBcluster5['overview_cluster'].value_counts())\n",
    "        #print(\"generalcluster\",TMDBcluster5['generalcluster'].value_counts())\n",
    "    if a==6:\n",
    "        TMDBcluster6=TMDB[TMDB['genre_cluster'] == a ]\n",
    "        tfidf_matrix_overview =  tfidf_vectorizer_overview.fit_transform(\n",
    "                         [x for x in TMDBcluster6[\"overview2\"]]) \n",
    "        km_overview = KMeans(n_clusters=subclustertotal)\n",
    "        km_overview.fit(tfidf_matrix_overview)\n",
    "        clusters_overview = km_overview.labels_.tolist()\n",
    "        TMDBcluster6[\"overview_cluster\"] = clusters_overview\n",
    "        subclusterCount6=TMDBcluster6[\"overview_cluster\"].max()\n",
    "        print(\"subclusterCount6=\",subclusterCount6)\n",
    "        TMDBcluster6[\"generalcluster\"]=clusters_overview\n",
    "        TMDBcluster6[\"generalcluster\"]+=(6+subclusterCount0+subclusterCount1+subclusterCount2\n",
    "                                            +subclusterCount3+subclusterCount4+subclusterCount5)\n",
    "        #print(\"overview_cluster\",TMDBcluster6['overview_cluster'].value_counts())\n",
    "        #print(\"generalcluster\",TMDBcluster6['generalcluster'].value_counts())\n",
    "    #df = pd.DataFrame(tfidf_matrix_overview.toarray(), columns = tfidf_vectorizer.get_feature_names())\n",
    "    #print(df) \n",
    "\n",
    "TMDB_updated=TMDBcluster0\n",
    "\n",
    "#print(TMDB_updated.info())\n",
    "#print(TMDBcluster1.info())\n",
    "TMDB_updated=TMDB_updated.append(TMDBcluster1)\n",
    "TMDB_updated=TMDB_updated.append(TMDBcluster2)\n",
    "TMDB_updated=TMDB_updated.append(TMDBcluster3)\n",
    "TMDB_updated=TMDB_updated.append(TMDBcluster4)\n",
    "TMDB_updated=TMDB_updated.append(TMDBcluster5)\n",
    "TMDB_updated=TMDB_updated.append(TMDBcluster6)\n",
    "\n",
    "print(TMDB_updated.info())\n",
    "print(TMDB_updated['generalcluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in range(TMDB_updated[\"generalcluster\"].max()+1):\n",
    "    \n",
    "    tfidf_vectorizerTEMP = TfidfVectorizer(max_df=0.9, max_features=20000,\n",
    "                                       min_df=0.05,stop_words='english',\n",
    "                                           tokenizer=tokenize_and_stem, use_idf=True,ngram_range=(1,3))\n",
    "    tfidf_transformerTEMP=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "\n",
    "    TMDBgeneralclusterTEMP=TMDB_updated[TMDB_updated['generalcluster'] == j]\n",
    "    tfidf_matrixTEMP =  tfidf_vectorizerTEMP.fit_transform([x for x in TMDBgeneralclusterTEMP[\"overview2\"]])\n",
    "    cvTEMP=CountVectorizer()\n",
    "    word_count_vectorTEMP=cvTEMP.fit_transform([a for a in TMDBgeneralclusterTEMP[\"overview2\"]])\n",
    "\n",
    "\n",
    "    tfidf_valuelistTEMP= tfidf_transformerTEMP.fit(word_count_vectorTEMP)\n",
    "\n",
    "    df_idfTEMP = pd.DataFrame(tfidf_valuelistTEMP.idf_, index=cvTEMP.get_feature_names(),columns=[\n",
    "        \"idf_weights\"])\n",
    "\n",
    "    print(\"cluster# \",j)\n",
    "    print(df_idfTEMP.sort_values(by=['idf_weights']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    print(\"Largest cluster number is \", TMDB_updated[\"generalcluster\"].max())\n",
    "    print(TMDB_updated[\"generalcluster\"].max()+1, \"Total Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TMDB_updated_old=TMDB_updated[TMDB_updated['year']< 2015]\n",
    "\n",
    "print(TMDB_updated_old.info())\n",
    "\n",
    "TMDB_updated_new=TMDB_updated[TMDB_updated['year']>= 2015]\n",
    "\n",
    "\n",
    "print(TMDB_updated[\"generalcluster\"].max())\n",
    "for i in range (TMDB_updated[\"generalcluster\"].max()):\n",
    "\n",
    "    TMDB_Subset_old=TMDB_updated_old[TMDB_updated_old['generalcluster']== i ]\n",
    "    TMDB_Subset_new=TMDB_updated_new[TMDB_updated_new['generalcluster']== i ]\n",
    "    TMDB_updated_subset=TMDB_updated[TMDB_updated['generalcluster']== i ]\n",
    "\n",
    "    \"\"\"  \n",
    "print(TMDB_updated[\"genre_cluster\"].max())\n",
    "for i in range (TMDB_updated[\"genre_cluster\"].max()+1):\n",
    "\n",
    "    TMDB_Subset_old=TMDB_updated_old[TMDB_updated_old['genre_cluster']== i ]\n",
    "    TMDB_Subset_new=TMDB_updated_new[TMDB_updated_new['genre_cluster']== i ]    \n",
    "    \n",
    "    \"\"\"     \n",
    "    \n",
    "    X_train=TMDB_Subset_old['year']\n",
    "    y_train=TMDB_Subset_old['profit'] \n",
    "    \n",
    "    X_train_full=TMDB_updated_subset['year']\n",
    "    y_train_full=TMDB_updated_subset['profit'] \n",
    "    \n",
    "    \n",
    "    X_test=TMDB_Subset_new['year']\n",
    "    y_test=TMDB_Subset_new['profit']\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #X_train,X_test,y_train,y_test = train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "    #below reshaping code required only when number of feature = 1 \n",
    "    X_train=X_train.values.reshape((-1,1))\n",
    "    y_train=y_train.values.reshape((-1,1))\n",
    "    X_train_full=X_train_full.values.reshape((-1,1))\n",
    "    y_train_full=y_train_full.values.reshape((-1,1))\n",
    "    \n",
    "    \n",
    "    X_test=X_test.values.reshape((-1,1))\n",
    "    y_test=y_test.values.reshape((-1,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #regold=LinearRegression().fit(X_train,y_train)\n",
    "    regtrain = LinearRegression().fit(X_train,y_train)\n",
    "    regactual = LinearRegression().fit(X_train_full,y_train_full)\n",
    "    \n",
    "    \n",
    "    #C = 100\n",
    "    #gamma = 10\n",
    "    #kernel = 'rbf'\n",
    "    #reg = SVC(C=C,gamma=gamma,kernel=kernel)\n",
    "    #regold.fit(X_train,y_train)   \n",
    "    regtrain.fit(X_train,y_train)\n",
    "    regactual.fit(X_train_full,y_train_full)\n",
    "    y_old= regtrain.predict(X_train)  #Lin reg of older data\n",
    "    y_pred = regtrain.predict(X_test) #predicted Lin reg of newer data, based on old data\n",
    "    y_regactual=regactual.predict(X_train_full) #actual Lin reg of newer data\n",
    "    \n",
    "    \"\"\"\n",
    "    polynomial_features= PolynomialFeatures(degree=2)\n",
    "    x_poly = polynomial_features.fit_transform(X_train)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_poly, y_train)\n",
    "    y_poly_pred = model.predict(x_poly)\n",
    "    \"\"\"\n",
    "    #polyfit = np.poly1d(np.polyfit(X_test,y_test,2))\n",
    "    \n",
    "    \n",
    "    #final_score = reg.score(y_pred, y_test)\n",
    "    print(\"Genre Cluster #: \",i)\n",
    "   \n",
    "    \n",
    "    plt.scatter(X_test, y_test, color ='g') \n",
    "    plt.scatter(X_train, y_train, color ='y')\n",
    "    \n",
    "    #Lin Regression prediction\n",
    "    plt.plot(X_test, y_pred, color ='r') \n",
    "    plt.plot(X_train, y_old, color ='r') \n",
    "    plt.plot(X_train_full, y_regactual, color ='k') \n",
    "    \"\"\"\n",
    "    sort_axis = operator.itemgetter(0)\n",
    "    sorted_zip = sorted(zip(X_train,y_poly_pred), key=sort_axis)\n",
    "    X_train, y_poly_pred = zip(*sorted_zip)\n",
    "    \n",
    "    \n",
    "    plt.plot(X_train, y_poly_pred, color='b')\n",
    "    #plt.plot(X_train, polyfit, color='p')\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.show()\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    print('----------')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
