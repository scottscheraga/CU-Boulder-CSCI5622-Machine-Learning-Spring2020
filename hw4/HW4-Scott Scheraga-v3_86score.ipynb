{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Kaggle Competition\n",
    "## CSCI 5622 - Spring 2019\n",
    "#Scott Scheraga\n",
    "***\n",
    "train-features.csv contains the features of the examples / cases, separated by commas, that you will use to train your classifiers. You must associate the features found in each row / line with the output found in the corresponding row / line in train-output.csv to create a full case.\n",
    "\n",
    "test-features.csv contains the features of the examples / cases that you will need to predict using your trained model.\n",
    "\n",
    "See train.names for header descriptions of the train and test features. This will be helpful in understanding how you may need to normalize, bin, or otherwise update your data to better suit your classifiers.\n",
    "\n",
    "You can use test-submission.demo to see what an example submission file should look like. This file is a baseline case - you can even upload it to make sure your Kaggle account is working correctly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching notes:\n",
    "Intial analysis of the training set showed the following information:\n",
    "from print(traincombined['output'].value_counts())\n",
    "        print(traincombined.groupby('output').mean())\n",
    "24720 people below threshold, 7841 above.\n",
    "\n",
    "\n",
    "As marital status averages were the following, I binned by married vs unmarried due to notably different outputs. \n",
    "\n",
    "--marital-status        |||         output                   \n",
    " Divorced                    0.104209  \n",
    " Married-AF-spouse           0.434783  \n",
    " Married-civ-spouse          0.446848  \n",
    " Married-spouse-absent       0.081340  \n",
    " Never-married               0.045961  \n",
    " Separated                   0.064390  \n",
    " Widowed                     0.085599  \n",
    " \n",
    "Similarly, for relationship, I binned by Spouse vs not spouse for the same reason as above, and then decided that the relationship column was redunant to marital status, and then deleted it. \n",
    "\n",
    "--Relationship      |||  output                    \n",
    " Husband              0.448571  \n",
    " Not-in-family        0.103070  \n",
    " Other-relative       0.037717  \n",
    " Own-child            0.013220  \n",
    " Unmarried            0.063262  \n",
    " Wife                 0.475128 \n",
    " \n",
    "Workclass:\n",
    "workclass      |||output                                          \n",
    " Self-emp-inc      0.557348\n",
    " Federal-gov       0.386458\n",
    " Local-gov         0.294792\n",
    " Self-emp-not-inc  0.284927\n",
    " State-gov         0.271957\n",
    " Private           0.218673\n",
    " ?                 0.104031\n",
    " Without-pay       0.000000\n",
    " Never-worked      0.000000\n",
    " \n",
    "There is a very notable increase in capital gain and loss for people above the income threshold. I decided on a 4 bins for capital gain and capital loss. \n",
    "\n",
    "--Averages for 0--\n",
    " age 36.783738 \n",
    " education-num 9.595065 \n",
    " capital-gain 148.752468\n",
    " capital-loss 53.142921 \n",
    " hours-per-week 38.840210  \n",
    "\n",
    "--Averages for 1--\n",
    " age 44.249841\n",
    " education-num 11.611657  \n",
    " capital-gain 4006.142456  \n",
    " capital-loss 195.001530 \n",
    " hours-per-week 45.473026 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self,features):\n",
    "        self.features=features\n",
    "        \n",
    "    def preprocess(self):    \n",
    "        \n",
    "        #self.features= pd.read_csv(\"traincombined.csv\")\n",
    "        \n",
    "        #https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "        self.features=self.features.drop(columns =\n",
    "                 ['native-country'])\n",
    "        \n",
    "        #self.features=self.features.drop(columns = \n",
    "          #  [\"workclass\",\"race\", \"educatio'DataFrame' object has no attribute 'ravel'n\",\"marital-status\",\"occupation\",\n",
    "         #    \"relationship\",\"race\",\"occupation\",\"sex\",\"fnlwgt\"])\n",
    "\n",
    "        \n",
    "        #print(self.features['education'].value_counts())\n",
    "        self.features['sex']=np.where(self.features['sex'] ==' Female',\n",
    "                                                1, self.features['sex'])\n",
    "        self.features['sex']=np.where(self.features['sex'] ==' Male',\n",
    "                                                0, self.features['sex'])\n",
    "        \n",
    "        #I might be losing data heere. Try splitting!\n",
    "        \"\"\"\n",
    "        self.features['education']=np.where(self.features['education'] ==' Preschool',\n",
    "                                                ' Some-school', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' 1st-4th',\n",
    "                                                ' Some-school', self.features['education'])       \n",
    "        self.features['education']=np.where(self.features['education'] ==' 5th-6th',\n",
    "                                                ' Some-school', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' 7th-8th',\n",
    "                                                ' Some-school', self.features['education'])  \n",
    "        \n",
    "        self.features['education']=np.where(self.features['education'] ==' 9th',\n",
    "                                                ' Some-school', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' 10th',\n",
    "                                                ' Some-school', self.features['education'])       \n",
    "        self.features['education']=np.where(self.features['education'] ==' 11th',\n",
    "                                                ' Some-school', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' 12th',\n",
    "                                                ' Some-school', self.features['education'])\n",
    "        \n",
    "        \n",
    "        self.features['education']=np.where(self.features['education'] ==' Assoc-voc',\n",
    "                                                ' Assoc-professional', self.features['education'])    \n",
    "        self.features['education']=np.where(self.features['education'] ==' Prof-school',\n",
    "                                                ' Assoc-professional', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' Assoc-acdm',\n",
    "                                                ' Assoc-professional', self.features['education'])        \n",
    "        self.features['education']=np.where(self.features['education'] ==' Some-college',\n",
    "                                                ' HS-grad or Some-college', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' HS-grad',\n",
    "                                                ' HS-grad or Some-college', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' Masters',\n",
    "                                                ' Graduate-degree', self.features['education'])\n",
    "        self.features['education']=np.where(self.features['education'] ==' Doctorate',\n",
    "                                                ' Graduate-degree', self.features['education'])\n",
    "       \n",
    "       \n",
    "        self.features['marital-status']=np.where(self.features['marital-status'] ==' Divorced',\n",
    "                                                ' Not Married', self.features['marital-status'])    \n",
    "        self.features['marital-status']=np.where(self.features['marital-status'] ==' Never-married',\n",
    "                                                ' Not Married', self.features['marital-status'])    \n",
    "        self.features['marital-status']=np.where(self.features['marital-status'] ==' Separated',\n",
    "                                                ' Not Married', self.features['marital-status'])    \n",
    "        self.features['marital-status']=np.where(self.features['marital-status'] ==' Widowed',\n",
    "                                                ' Not Married', self.features['marital-status'])  \n",
    "        self.features['marital-status']=np.where(self.features['marital-status'] ==' Married-spouse-absent',\n",
    "                                                ' Not Married', self.features['marital-status'])        \n",
    "        self.features['marital-status']=np.where(self.features['marital-status'] ==' Married-civ-spouse',\n",
    "                                                ' Married', self.features['marital-status'])    \n",
    "        self.features['marital-status']=np.where(self.features['marital-status'] ==' Married-AF-spouse',\n",
    "                                                ' Married', self.features['marital-status'])  \n",
    "        \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Never-worked',\n",
    "                                                ' Zero', self.features['workclass']) \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Without-pay',\n",
    "                                                ' Zero', self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' ?',\n",
    "                                                ' Low', self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Private',\n",
    "                                                ' Low', self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' State-gov',\n",
    "                                                ' Mid', self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Self-emp-not-inc',\n",
    "                                                ' Mid', self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Local-gov',\n",
    "                                                ' Mid', self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Federal-gov',\n",
    "                                                ' High', self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Self-emp-inc',\n",
    "                                                ' High', self.features['workclass'])  \n",
    "        \n",
    "        #Label Encoding\n",
    "        \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Never-worked',\n",
    "                                                1, self.features['workclass']) \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Without-pay',\n",
    "                                                1, self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' ?',\n",
    "                                                2, self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Private',\n",
    "                                                2, self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' State-gov',\n",
    "                                                3, self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Self-emp-not-inc',\n",
    "                                                3, self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Local-gov',\n",
    "                                                3, self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Federal-gov',\n",
    "                                                4, self.features['workclass'])  \n",
    "        self.features['workclass']=np.where(self.features['workclass'] ==' Self-emp-inc',\n",
    "                                                4, self.features['workclass']) \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #A lot of people dumped coutnry for good or bad...\n",
    "        #0 mean output from test set\n",
    "        country1 = [' Holand-Netherlands', ' Outlying-US(Guam-USVI-etc)']\n",
    "        #less than 0.1 mean output from test set\n",
    "        country2 = [' Dominican-Republic',' Columbia',' Guatemala',' Mexico', \n",
    "                    ' Nicaragua',' Peru',' Vietnam', ' Honduras',' Guatemala',' El-Salvador',' Haiti']\n",
    "        # 0.1 to 0.2 mean output from test set\n",
    "        country3 = [' Puerto-Rico',' Trinadad&Tobago',' Portugal',' Laos', \n",
    "                    ' Jamaica',' Ecuador',' Thailand']\n",
    "        #0.2-0.3 mean output from test set\n",
    "        country4 = [' Poland',' South',' Ireland',' Hungary',' United-States',\n",
    "                    ' Scotland',' ?',' Ecuador',' Cuba',' China',' Greece'] \n",
    "        #0.3-0.4 mean output from test set\n",
    "        country5 = [' Hong',' Philippines',' Germany',' Canada',' England',\n",
    "                    ' Italy',' Cambodia',' Yugoslavia',' Japan',' Taiwan']         \n",
    "        #greater than 0.4 mean output from test set\n",
    "        country6 = [' India',' France',' Iran']      \n",
    "        \n",
    "        \"\"\"  \n",
    "        for x in country1:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                ' countrygroup1', self.features['native-country']) \n",
    "        for x in country2:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                ' countrygroup2', self.features['native-country']) \n",
    "        for x in country3:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                ' countrygroup3', self.features['native-country'])         \n",
    "        for x in country4:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                ' countrygroup4', self.features['native-country']) \n",
    "        for x in country5:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                ' countrygroup5', self.features['native-country'])         \n",
    "        for x in country6:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                ' countrygroup6', self.features['native-country'])          \n",
    "        \n",
    "        #Label Encoding\n",
    "        \n",
    "        for x in country1:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                1, self.features['native-country']) \n",
    "        for x in country2:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                2, self.features['native-country']) \n",
    "        for x in country3:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                3, self.features['native-country'])         \n",
    "        for x in country4:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                4, self.features['native-country']) \n",
    "        for x in country5:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                5, self.features['native-country'])         \n",
    "        for x in country6:\n",
    "            self.features['native-country']=np.where(self.features['native-country'] == x,\n",
    "                                                6, self.features['native-country'])          \n",
    "             \n",
    "        \"\"\"\n",
    "        \n",
    "        #https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9\n",
    "        #https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "        #Normalization of continuous data\n",
    "        \n",
    "        #self.features['education-num'] = preprocessing.scale(self.features['education-num'])\n",
    "        #self.features[\"fnlwgt\"] = preprocessing.scale(self.features[\"fnlwgt\"])\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        normscalefeatures=['capital-gain','capital-loss','education-num',\"hours-per-week\",\"fnlwgt\",\"age\"]\n",
    "        for n in normscalefeatures:\n",
    "           # self.features[n] = preprocessing.normalize(self.features[n])\n",
    "            self.features[n] = preprocessing.scale(self.features[n])\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        #scaler1 = MinMaxScaler(feature_range = (0,1))\n",
    "        #scaler1.fit_transform(self.features['capital-gain'].values.reshape(-1, 1))\n",
    "        #scaler2 = MinMaxScaler(feature_range = (0,1))\n",
    "        #scaler2.fit_transform(self.features['capital-loss'].values.reshape(-1, 1))\n",
    "        #scaler3 = MinMaxScaler()\n",
    "        #scaler3.fit_transform(self.features['age'].values.reshape(-1, 1))\n",
    "        #scaler4 = MinMaxScaler(feature_range = (0,1))\n",
    "        #scaler4.fit_transform(self.features['education-num'].values.reshape(-1, 1))\n",
    "        #scaler5 = MinMaxScaler()\n",
    "        #scaler5.fit_transform(self.features['fnlwgt'].values.reshape(-1, 1))\n",
    "\n",
    "        \n",
    "     \n",
    "        #self.features['capital-gain']=np.where(self.features['capital-gain'].astype(float) < 149,\n",
    "          #                                      ' Low', self.features['capital-gain'])\n",
    "        \n",
    "        #self.features['capital-gain']=np.where((self.features['capital-gain'].astype(float) > 149) &\n",
    "        #         (self.features['capital-gain'].astype(float) <2077),' Mid-Low', self.features['capital-gain'])        \n",
    "        #self.features['capital-gain']=np.where((self.features['capital-gain'] > 2077) &\n",
    "           #      (self.features['capital-gain'] <4006),' Mid-High', self.features['capital-gain']) \n",
    "       \n",
    "        #self.features['capital-gain']=np.where(int(self.features['capital-gain']) > 4006,\n",
    "        #                                        ' High', self.features['capital-gain'])        \n",
    "\n",
    "        \"\"\"\n",
    "        capital losses: 0, 53.142921, 124.0722255, 195.001530, and up\n",
    "        \"\"\"\n",
    "\n",
    "        #Reference: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "\n",
    "        self.features.insert(2,\"capitalchange\",self.features['capital-loss']+self.features['capital-gain'])\n",
    "        #self.features['capitalchange']=\n",
    "        \n",
    "        #print(self.features)\n",
    "        category_variables=['marital-status',\"sex\",\"race\",'workclass','education',\n",
    "                            \"relationship\",\"occupation\"] #'education',,'workclass'\n",
    "        for var in category_variables:\n",
    "            cat_list='var'+'_'+var\n",
    "            cat_list = pd.get_dummies(self.features[var], prefix=var)\n",
    "            data1=self.features.join(cat_list)\n",
    "            self.features=data1\n",
    "            \n",
    "        category_variables=['marital-status',\"race\",\"sex\",'workclass','education',\n",
    "                            \"relationship\",\"occupation\"] #,\"relationship\"'education',\n",
    "        self.features_vars=self.features.columns.values.tolist()\n",
    "        to_keep=[i for i in self.features_vars if i not in category_variables]\n",
    "        \n",
    "        \n",
    "        \n",
    "        data_final=self.features[to_keep]\n",
    "        #data_final=data_final.drop(columns = [   \n",
    "         #   'education_ Graduate-degree','education_ HS-grad or Some-college',\n",
    "         #   'education_ Assoc-professional','education_ Bachelors']) \n",
    "        \n",
    "        \n",
    "        #'workclass_ Low','workclass_ Mid','education_ Bachelors'\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #Having just White and Black races seems to improve the model the best\n",
    "        data_final=data_final.drop(columns =['race_ Amer-Indian-Eskimo','race_ Asian-Pac-Islander',\n",
    "                                        'race_ Other','capital-gain','capital-loss'])#'race_ Black',\n",
    "        \n",
    "        \n",
    "        \n",
    "                    \n",
    "        #(\n",
    "        \n",
    "        \"\"\"\n",
    "        data_final=data_final.drop(columns =[ 'occupation_ Farming-fishing','occupation_ Handlers-cleaners',\n",
    "        'occupation_ Machine-op-inspct','occupation_ Priv-house-serv','occupation_ Protective-serv' ,\n",
    "                                    'occupation_ Tech-support' ,'occupation_ Transport-moving'])\n",
    "        \n",
    " \n",
    " \n",
    "        data_final.columns.values\n",
    "        \n",
    "        #print(data_final)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return data_final\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 62)\n",
      "(16281, 62)\n",
      "(32561, 62)\n",
      "(16281, 62)\n",
      "(32561, 62)\n",
      "(16281, 62)\n",
      "Epoch #: 0  Score:  0.8626704336076649   Train Score:  0.8695968695968695\n",
      "Epoch #: 1  Score:  0.8637759489006265   Train Score:  0.8709163709163709\n",
      "Epoch #: 2  Score:  0.8634074438029726   Train Score:  0.8693238693238693\n",
      "Epoch #: 3  Score:  0.864881464193588   Train Score:  0.8696878696878697\n",
      "Epoch #: 4  Score:  0.8646357941284855   Train Score:  0.8691873691873692\n",
      "Epoch #: 5  Score:  0.8643901240633829   Train Score:  0.8693238693238693\n",
      "Epoch #: 6  Score:  0.8642672890308316   Train Score:  0.8705978705978706\n",
      "Epoch #: 7  Score:  0.8642672890308316   Train Score:  0.8710983710983711\n",
      "Epoch #: 8  Score:  0.8637759489006265   Train Score:  0.868004368004368\n",
      "Epoch #: 9  Score:  0.8634074438029726   Train Score:  0.8702338702338702\n",
      "   \n",
      "FINAL SCORE:  0.864881464193588\n",
      "   \n",
      "R2 score: 0.24640952044805242\n",
      "Mean squared error: 0.1365925561970274\n",
      "Variance 0.11793502978858929\n",
      "   \n",
      "[0 0 0 ... 1 0 1]\n",
      "Data Saved to file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindatafeatures= pd.read_csv(\"train-features_scottannotated.csv\")\n",
    "trainoutput= pd.read_csv(\"train-output_scottannotated.csv\")\n",
    "testdatafeatures= pd.read_csv(\"test-features_scottannotated.csv\")  \n",
    "\n",
    "#Fill in missing data:\n",
    "catfeatures=['workclass','education','marital-status','occupation',\n",
    "             'relationship','race','sex','native-country',]\n",
    "\n",
    "continfeatures=['age','fnlwgt','education-num','capital-gain',\n",
    "                'capital-loss','hours-per-week']\n",
    "\n",
    "#NAs = pd.concat([testdatafeatures.isnull().sum()], axis=1,keys=['Train']) \n",
    "#print(NAs[NAs.sum(axis=1) > 0])\n",
    "#NAs = pd.concat([train.isnull().sum()], axis=1, keys=[‘Train’])\n",
    "#NAs[NAs.sum(axis=1) > 0]\n",
    "\n",
    "for a in continfeatures:\n",
    "        traindatafeatures[a] = traindatafeatures[a].fillna(int(traindatafeatures[a].mean()))\n",
    "        testdatafeatures[a] = testdatafeatures[a].fillna(int(testdatafeatures[a].mean()))\n",
    "for b in catfeatures:\n",
    "        traindatafeatures[b] = traindatafeatures[b].fillna(traindatafeatures[b].mode()[0])\n",
    "        testdatafeatures[b] = testdatafeatures[b].fillna(testdatafeatures[b].mode()[0])\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "data_train = Data(traindatafeatures).preprocess()\n",
    "data_test= Data(testdatafeatures).preprocess()\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "\n",
    "#print(data_train.info())\n",
    "\n",
    "#print(data_test.info())\n",
    "\n",
    "#Data_train= (32561, 45)\n",
    "#Data_Test= (16281, 45)\n",
    "\n",
    "combinedData = pd.concat([data_train,data_test])\n",
    "\n",
    "normscalefeatures=[\"age\",'education-num']\n",
    "for n in normscalefeatures:\n",
    "        # self.features[n] = preprocessing.normalize(self.features[n])\n",
    "        #data_train[n] = preprocessing.scale(data_train[n])\n",
    "        #data_test[n] = preprocessing.scale(data_test[n])\n",
    "        combinedData[n] = preprocessing.scale(combinedData[n])\n",
    "\n",
    "\"\"\" \"\"\"       \n",
    "minmaxscalefeatures=[\"capitalchange\",\"fnlwgt\",\"hours-per-week\"]\n",
    "for n in minmaxscalefeatures:\n",
    "        # self.features[n] = preprocessing.normalize(self.features[n])\n",
    "        #data_train[n] = preprocessing.scale(data_train[n])\n",
    "        #data_test[n] = preprocessing.scale(data_test[n])\n",
    "        scaler1 = MinMaxScaler(feature_range = (0,1))\n",
    "        combinedData[n]= scaler1.fit_transform(combinedData[n].values.reshape(-1, 1))        \n",
    "\n",
    "dfs = np.split(combinedData, [32561], axis=0)\n",
    "data_train =dfs[0]\n",
    "data_test= dfs[1]\n",
    "print(dfs[0].shape)\n",
    "print(dfs[1].shape)        \n",
    "        \n",
    "#print(trainoutput)\n",
    "trainoutputArray=np.asarray(trainoutput)\n",
    "np.transpose(trainoutputArray)\n",
    "\n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(7)\n",
    "n_samples, n_features = data_train.shape\n",
    "n_samplestest, n_featurestest = data_test.shape\n",
    "data_train = np.c_[data_train, random_state.randn(n_samples, 0)]\n",
    "data_test = np.c_[data_test, random_state.randn(n_samplestest, 0)]\n",
    "\"\"\"\"\"\"\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "#data_test.preprocess()\n",
    "\n",
    "#print(trainoutputArray.ravel())\n",
    "\n",
    "#data_final_vars=data_train.columns.values.tolist()\n",
    "#print(data_train.columns.values)\n",
    "\n",
    "\n",
    "x_train, x_holdout, y_train, y_holdout = train_test_split(\n",
    "             data_train, trainoutputArray.ravel(), test_size=0.25, random_state=10)\n",
    "\n",
    "#https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4\n",
    "\"\"\"\n",
    "model = ensemble.GradientBoostingRegressor(n_estimators = 100, max_depth = 2,\n",
    "                             min_samples_split = 2, learning_rate = .01,loss = 'ls')\n",
    "bestmodel = ensemble.GradientBoostingRegressor(n_estimators = 100, max_depth = 2,\n",
    "                             min_samples_split = 2, learning_rate = .01,loss = 'ls')\n",
    "\"\"\"           \n",
    "model=ensemble.GradientBoostingClassifier()\n",
    "bestmodel=ensemble.GradientBoostingClassifier()\n",
    "#model= AdaBoostClassifier(n_estimators=150)\n",
    "#bestmodel = AdaBoostClassifier(n_estimators=150)\n",
    "\n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\n",
    "\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "#https://towardsdatascience.com/cross-validation-a-beginners-guide-5b8ca04962cd\n",
    "trainindexcounter=0\n",
    "bestscore=0\n",
    "score=0\n",
    "bestmodel\n",
    "kf = KFold(n_splits=10) # Define the split - into 2 folds \n",
    "#kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "for train_index, test_index in kf.split(x_train):\n",
    "    x_trainfold, x_testfold = x_train[train_index], x_train[test_index]\n",
    "    y_trainfold, y_testfold = y_train[train_index], y_train[test_index]\n",
    "    model.fit(x_trainfold, y_trainfold)\n",
    "    score = model.score(x_holdout, y_holdout)\n",
    "    trainscore = model.score(x_trainfold, y_trainfold)\n",
    "    print(\"Epoch #:\", trainindexcounter, \" Score: \",score, \"  Train Score: \",trainscore) \n",
    "    trainindexcounter+=1\n",
    "    \n",
    "    if score>bestscore:\n",
    "        bestscore=score\n",
    "        bestmodel=model\n",
    "    #if trainindexcounter==1:\n",
    "      #  break\n",
    "    \n",
    "score=bestscore    \n",
    "model=bestmodel\n",
    "#print(confusion_matrix(y_holdout, modeloutput.predict(x_holdout)))\n",
    "finalpredictions=model.predict(x_holdout)\n",
    "#predictions= model.predict(data_test)\n",
    "#print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "#tn, fp, fn, tp = confusion_matrix(y_holdout, finalpredictions).ravel()\n",
    "\n",
    "print (\"   \")\n",
    "print(\"FINAL SCORE: \",score)\n",
    "#print(\"tn=\",tn,\" fp=\" ,fp,\" fn=\", fn,\" tp=\", tp)\n",
    "print (\"   \")\n",
    "print(\"R2 score:\",  r2_score(y_holdout,finalpredictions))\n",
    "print(\"Mean squared error:\", mean_squared_error(\n",
    "              y_holdout,finalpredictions))\n",
    "\n",
    "#variance calc\n",
    "#https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/\n",
    "er = []\n",
    "g = 0\n",
    "for i in range(len(y_holdout)):\n",
    "    #print( \"actual=\", y_holdout[i], \" observed=\", finalpredictions[i])\n",
    "    x = (y_holdout[i] - finalpredictions[i]) **2\n",
    "    er.append(x)\n",
    "    g = g + x\n",
    "v = np.var(er)\n",
    "\n",
    "print (\"Variance\", v)\n",
    "print (\"   \")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "ns_probs = [0 for _ in range(len(y_holdout))]\n",
    "ns_auc = roc_auc_score(y_holdout, ns_probs)\n",
    "\n",
    "lr_probs=model.predict_proba(x_holdout)\n",
    "lr_probs = lr_probs[:, 1]\n",
    "fpr, tpr,_=roc_curve(y_holdout,lr_probs) \n",
    "lr_auc= roc_auc_score( y_holdout,finalpredictions)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "##Adding the ROC\n",
    "plt.plot(fpr, tpr, color='red', lw=2,label='ROC curve') #lw=2,\n",
    "##Random FPR and TPR\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "##Title and label\n",
    "plt.xlabel('FPR- False Positive')\n",
    "plt.ylabel('TPR-True Positive')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"AUC No Skill=\",ns_auc)\n",
    "print(\"AUC Logisitic=\",lr_auc)\n",
    "\"\"\"\n",
    "\n",
    "#https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "\n",
    "\n",
    "predictions= model.predict(data_test)\n",
    "print(predictions)\n",
    "    \n",
    "        # dictionary of lists  \n",
    "dict = {'Category': predictions }  \n",
    "   \n",
    "df = pd.DataFrame(dict) \n",
    "  \n",
    "        # saving the dataframe \n",
    "df.to_csv('ScottSubmission3.csv') \n",
    "print(\"Data Saved to file\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "#Professor suggests I look into:\n",
    "#ensembles : random forest, etc  or neural nets\n",
    "#svm, neural nets and random forest have more variables I can tweak thant log regression\n",
    "\n",
    "#naive bayes-  potentially not great..\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "#https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
    "#https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
