{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Clustering\n",
    "## CSCI 5622 - Spring 2020\n",
    "***\n",
    "**Name**: $<$insert name here$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11.59 PM on Monday, April 27th**. Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your classmates and instructors, but **you must write all code and solutions on your own**, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIris:\n",
    "    def __init__(self):\n",
    "        data = load_iris()\n",
    "        self.data = data.data[:, [1, 3]]\n",
    "        self.target = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 [65 Pts]: K-means clustering\n",
    "\n",
    "\n",
    "> First, we're going to build an implementation of K-means clustering as discussed in lecture. We will cluster a reduced form of the Iris dataset with a dimensionality of $d=2$ found in class `IrisData` above. <br>\n",
    "\n",
    "Tasks :\n",
    "Implement K-means clustering by completing the code in the class below : \n",
    "\n",
    "- [10 points] `initialize_centroids` : select K distinct points from the dataset `X` passed in the constructor and use them as the initial centroids. Store these centroids in the class variable `self.centroids` as an `np.array` of shape $K \\times d$ where the $i$-th row represents a centroid and its would be class label is also $i$\n",
    "- [10 points] `compute_distances` : compute the distance of each point $x_i$ to every centroid $m_k$ and return the result as a matrix `distance_matrix` of size $N \\times K$ where N is the number of points and K is the chosen number of clusters to be found. A cell `(i,k)` shall contain the euclidean distance between point $x_i$ and centroid $m_k$\n",
    "- [10 points] `compute_cluster_assignment` : given the distance matrix of size $N \\times K$ return a array of labels in which each element is an integer in the range $[0, K-1]$ and it represents which centroid in the centroid array `self.centroids` that point belongs to.\n",
    "- [10 points] `compute_centroids` : You shall have the new assignment of clusters contained in the incoming `labels` array to your $N$ points. Compute the new centroids depending on the new set of points that has been alloted to each cluster. \n",
    "- [10 points] `cluster` : This shall contain you main loop which implements the algorithm described above. You shall sequentially call the methods above to find the $K$ centroids(or means). Keep performing this loop until one of the following conditions is met: 1) you perform `epochs` number of iterations; or 2) the average euclidian difference between old and new centroids goes below 0.01. At the end of each epoch call the method `show_progress()` to show where the centroids are after making the necessary updates.\n",
    "- [The Elbow Analysis function is used in the bonus problem below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    def __init__(self, X, K):\n",
    "        '''\n",
    "        Params : \n",
    "            X : (np.ndarray) of dimension (N, d) N is the number of points\n",
    "            K : (int) number of means/centroids to evaluate\n",
    "            epochs : (int) maximum number of epochs to evaluate for the centroids\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.K = K\n",
    "        self.centroids = self.initialize_centroids()\n",
    "        \n",
    "    def initialize_centroids(self):\n",
    "        '''\n",
    "        Randomly select K distinct points from the dataset in self.X\n",
    "        Params : \n",
    "            None\n",
    "        RETURN :\n",
    "            means : (np.ndarray) of the dimension (K, d)\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def compute_distances(self):\n",
    "        '''\n",
    "        Comupute a distance matrix of size (N, K) where each cell (i, j) represents the distance between \n",
    "        i-th point and j-th centroid. We shall use Euclidean distance here.\n",
    "        \n",
    "        PARAMS:\n",
    "            centroids : (np.ndarray) of the dimension (K, d)\n",
    "        RETURN:\n",
    "            distance_matrix : (np.ndarray) of the dimension (N, K)\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def compute_cluster_assignment(self, distance_matrix):\n",
    "        '''\n",
    "        Comupute a distance matrix of size (N, K) where each cell (i, j) represents the distance between \n",
    "        i-th point and j-th centroid. We shall use Euclidean distance here.\n",
    "        \n",
    "        PARAMS:\n",
    "            distance_matrix : (np.ndarray) of the dimension (N, K)\n",
    "        RETURN:\n",
    "            labels : (np.ndarray) of the size (N)\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def compute_centroids(self, labels):\n",
    "        '''\n",
    "        Randomly select K distinct points from the dataset in self.X\n",
    "        Params : \n",
    "            labels : (np.ndarray) of the dimension (N) where each i-th item reperesents the closest\n",
    "            centroid among the K centroids. Each value here must be between 0 and K-1.\n",
    "        RETURN :\n",
    "            updated_means : (np.ndarray) of the dimension (K, d)\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def cluster(self, epochs):\n",
    "        '''\n",
    "        Implement the K-means algorithm here as described above. However loop for a maximum of self.epochs.\n",
    "        Ensure that you have a condition that checks whether the epochs have changed since the last epoch or not\n",
    "        For this use a threshold of change of 0.01.\n",
    "        \n",
    "        PARAMS:\n",
    "            epochs : (integer) maximum number of epochs\n",
    "        RETURN:\n",
    "            centroids : (np.ndarray) of the size (K, d) also store in a class variable self.centroids\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def show_progress(self, epoch):\n",
    "        '''\n",
    "        PARAMS:\n",
    "            epoch : (integer) tells which epoch is it\n",
    "        RETURN:\n",
    "            None\n",
    "        '''\n",
    "        plt.plot(self.X[:, 0], self.X[:, 1], 'o', color='y')\n",
    "        for i in range(self.K):\n",
    "            plt.plot(self.centroids[i, 0], self.centroids[i, 1], 'o', color='k')\n",
    "        plt.title('Centroids at epoch : {}'.format(epoch))\n",
    "        plt.show()\n",
    "        \n",
    "    def elbow_analysis(self, k_range):\n",
    "        '''\n",
    "        PARAMS:\n",
    "            k_range : (list of +ve integers) contains the K number of hyperparameters k to peform \n",
    "            the analysis over\n",
    "        RETURN:\n",
    "            avg_variance : (list of float) list od size K. contains the average variance of clusters corresponding to each \n",
    "            to each hyperparameter k\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EEEE\n",
      "======================================================================\n",
      "ERROR: test_compute_centroids (__main__.KMeansTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-3331454b3a7a>\", line 5, in setUp\n",
      "    self.X = np.array([[0.1, 0.3], [0.4, 0.6], [0.2, 0.4], [3.1, 3.1], [3.5, 2.9]])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_compute_distances (__main__.KMeansTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-3331454b3a7a>\", line 5, in setUp\n",
      "    self.X = np.array([[0.1, 0.3], [0.4, 0.6], [0.2, 0.4], [3.1, 3.1], [3.5, 2.9]])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_initialize_centroids (__main__.KMeansTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-3331454b3a7a>\", line 5, in setUp\n",
      "    self.X = np.array([[0.1, 0.3], [0.4, 0.6], [0.2, 0.4], [3.1, 3.1], [3.5, 2.9]])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: tests_compute_cluster_assignment (__main__.KMeansTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-3331454b3a7a>\", line 5, in setUp\n",
      "    self.X = np.array([[0.1, 0.3], [0.4, 0.6], [0.2, 0.4], [3.1, 3.1], [3.5, 2.9]])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "FAILED (errors=4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=4 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class KMeansTester(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.X = np.array([[0.1, 0.3], [0.4, 0.6], [0.2, 0.4], [3.1, 3.1], [3.5, 2.9]])\n",
    "        self.cluster_obj_1 = KMeansClustering(self.X, 2)\n",
    "        self.cluster_obj_2 = KMeansClustering(self.X, 2)\n",
    "        self.cluster_obj_3 = KMeansClustering(self.X, 2)\n",
    "        self.cluster_obj_4 = KMeansClustering(self.X, 2)\n",
    "        \n",
    "    def test_initialize_centroids(self):\n",
    "        \"\"\"\n",
    "        Test initialize_centroids function from KMeansClustering\n",
    "        \"\"\"\n",
    "        means = self.cluster_obj_1.initialize_centroids()\n",
    "        self.assertEqual(means.shape[0], 2)\n",
    "        self.assertEqual(means.shape[1], 2)\n",
    "        \n",
    "    def test_compute_distances(self):\n",
    "        \"\"\"\n",
    "        Test compute_distances function from KMeansClustering\n",
    "        \"\"\"\n",
    "        self.cluster_obj_2.centroids = self.X[:2, :]\n",
    "        distance_matrix = self.cluster_obj_2.compute_distances()\n",
    "        self.assertEqual(round(distance_matrix[0,0], 2), 0.0)\n",
    "        self.assertEqual(round(distance_matrix[0,1], 2), 0.42)\n",
    "        \n",
    "    def tests_compute_cluster_assignment(self):\n",
    "        \"\"\"\n",
    "        Test compute_cluster_assignment function from KMeansClustering\n",
    "        \"\"\"\n",
    "        self.cluster_obj_3.centroids = self.X[:2, :]\n",
    "        distance_matrix = self.cluster_obj_3.compute_distances()\n",
    "        labels = self.cluster_obj_3.compute_cluster_assignment(distance_matrix)\n",
    "        self.assertEqual(labels[0], 0)\n",
    "        self.assertEqual(labels[1], 1)\n",
    "        self.assertEqual(labels[2], 0)\n",
    "        \n",
    "    def test_compute_centroids(self):\n",
    "        \"\"\"\n",
    "        Test compute_centroids function from KMeansClustering\n",
    "        \"\"\"\n",
    "        self.cluster_obj_4.centroids = self.X[:2, :]\n",
    "        distance_matrix = self.cluster_obj_4.compute_distances()\n",
    "        labels = self.cluster_obj_4.compute_cluster_assignment(distance_matrix)\n",
    "        new_means = self.cluster_obj_4.compute_centroids(labels)\n",
    "        self.assertEqual(round(new_means[0, 0], 2), 0.15)\n",
    "        self.assertEqual(round(new_means[0, 1], 2), 0.35)\n",
    "        \n",
    "    \n",
    "tests = KMeansTester()\n",
    "myTests = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(myTests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [5 pts] In the cell below, perform K-means clustering on the `IrisData` with `K=3` for up to __20 epochs__. Plot the centroid locations at each epoch using the method `show_progress` already written for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [10 pts] The original Iris dataset was a labelled dataset with classes assigned to each point. The `show_progress` method plots all the points as yellow without giving points of different classes different colors along with the centroids as black points. Now that you have performed clustering, try to plot the points in the dataset along with the computed centroids, but this time color the dataset points according to their true label in `IrisData` (see the attribute `target` for true class labels in `IrisData`). Comment on what you observe in the Markdown cell following the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Analysis [Bonus 15 Pts]\n",
    "\n",
    "Our Iris dataset should have an expectation of three clusters (as seen in the match to the targets?), but we want to see if that is borne out by an Elbow Test. Add a function tpdate the elbow_analysis function in your KMeansClustering Class above to compute the average within-cluster variance, run this within cluster variance for k = 1 to k = 10 and plot the results below, and discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_write your response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D data from Gaussian Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data1D:\n",
    "    def __init__(self):\n",
    "        self.means = [-4.0, 3.5, 10.6]\n",
    "        self.variances = [1.5, 1.2, 1.0]\n",
    "        X = []\n",
    "        for m, v in zip(self.means, self.variances):\n",
    "            X += list(np.random.normal(m, np.sqrt(v), size=(100)))\n",
    "        self.means, self.variances = np.array(self.means), np.array(self.variances)\n",
    "        X = np.array(X)\n",
    "        self.X = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 [35 Pts]: Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMMs are based on the assumption that all data points come from a fine mixture of Gaussian distributions with unknown parameters. They are parametric generative models that attempt to learn the true data distribution. Hence, once we learn the Gaussian parameters, we can generate data from the same distribution as the source. <br>\n",
    "\n",
    "Like K-Means, GMMs also demand the number of clusters K as an input to the learning algorithm. However, there is a key difference between the two. K-Means can only learn clusters with a circular form. GMMs, on the other hand, can learn clusters with any elliptical shape. <br>\n",
    "\n",
    "K-Means only allows for an observation to belong to one, and only one cluster. Differently, GMMs give probabilities that relate each example with a given cluster. In other words, GMMs allow for an observation to belong to more than one cluster â€” with a level of uncertainty. <br>\n",
    "\n",
    "Assuming one-dimensional data(as is the data for the problem we will solve) and the number of clusters K equals 3, GMMs attempt to learn 9 parameters.\n",
    "- 3 parameters for the means i.e $\\mu_k$\n",
    "- 3 parameters for the variances i.e $\\sigma^2_k$\n",
    "- 3 scaling parameters $\\phi_k$\n",
    "\n",
    "We estimate these parameters using Expectation Maximization which works as follows:\n",
    "1. First we calculate the likelihood of each point. Below is the Probability Density Function we will use to evaluate the likelihood\n",
    "$$f(x|\\mu_k, \\sigma^2_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}exp\\Big(-\\frac{(x-\\mu_k)^2}{2\\sigma^2_k}\\Big)$$\n",
    "\n",
    "2. Next we estimate the posterior probability of each point with repect to certain cluster. This tells us what is the probability that a certain point belongs to a certain cluster\n",
    "$$b_k^{(i)} = \\frac{f(x^{(i)}|\\mu_k, \\sigma^2_k)\\phi_k}{\\sum_{k=1}^{K}f(x^{(i)}|\\mu_k, \\sigma^2_k)\\phi_k}$$\n",
    "\n",
    "3. Now we estimate the parameters as\n",
    "$$\\mu_k = \\frac{\\sum_i b_k^{(i)}x^{(i)}}{\\sum_i b_k^{(i)}}$$\n",
    "$$\\sigma^2_k = \\frac{\\sum_i b_k^{(i)}(x^{(i)} - \\mu_k)^2}{\\sum_i b_k^{(i)}}$$\n",
    "$$\\phi_k = \\frac{1}{N}\\sum_i b_k^{(i)}$$\n",
    "\n",
    "4. We repeat the above steps until the parameters dont change beyond a certain threshold or we reach the maximum number of epochs\n",
    "\n",
    "> Now we shall implement Gaussian Mixture Model for a 1-D dataset composed of points from multiple Gaussian Distributions. You can find the data in `Data1D` above.\n",
    "> Complete the methods below : \n",
    "- [5 points] `compute_pdf` : given a scalar value x(a 1-D point) and integer k find the value of the Probability Density Function(p.d.f) as defined above\n",
    "- [5 points] `compute_pdf_matrix` : Create a matrix of size $N\\times K$ which contains the p.d.f value for every  point w.r.t each cluster\n",
    "- [5 points] `compute_posterior` : Given the p.d.f matrix computed from previous function, now create a matrix of size $N\\times K$ which contains the posterior probability value for every  point w.r.t each cluster as using the formula described above\n",
    "- [5 points] `reestimate_params` : Given the posterior probability matrix, re-estimate the 3 sets of parameters \n",
    "- [5 points] `exp_maximize` : the main training loop that runs for `epochs` number of times. You must also incorporate logic here to break the loop if the parameters do not change beyond a certain threshold just like you did before in K-means. We give you the liberty of deciding how that will work and what the threshold value should be. Note that finally the parameters should be in close range with the parmaters the data distribution was created with in class `Data1D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel1D:\n",
    "    def __init__(self, X, K):\n",
    "        self.X = X\n",
    "        self.K = K\n",
    "        self.mean, self.variance, self.weight = self.initialize_parameters()\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        mean = np.random.choice(self.X, self.K)\n",
    "        variance = np.random.random_sample(size=self.K) * 2\n",
    "        weights = np.ones(self.K) / self.K\n",
    "        return mean, variance, weights\n",
    "    \n",
    "    def compute_pdf(self, x, k):\n",
    "        '''\n",
    "        Evaluate the p.d.f value for 1-D point i.e scalar value for the w.r.t to the k-th cluster\n",
    "        Params : \n",
    "            x : (float) the point\n",
    "            k : (integer) the k-th elements from mean, variance and weights correspond to k-th cluster parameters.\n",
    "                Use those to estimate your result.\n",
    "        RETURN :\n",
    "            result : (float) evalutated using the formula described above\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "    def compute_pdf_matrix(self):\n",
    "        '''\n",
    "        Evaluate the p.d.f martix by calling compute_pdf() for each combination of x and k\n",
    "        Params : \n",
    "            None\n",
    "        RETURN :\n",
    "            result : (np.array) matrix of size N X K containing p.d.f values\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def compute_posterior(self, pdf_matrix):\n",
    "        '''\n",
    "        Evaluate the posterior probability martix as described by the formula above\n",
    "        Params : \n",
    "            pdf_matrix : (np.array) matrix of size N X K containing p.d.f values\n",
    "        RETURN :\n",
    "            result : (np.array) matrix of size N X K containing posterior probability values\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def reestimate_params(self, posterior_matrix):\n",
    "        '''\n",
    "        Re-estimate the cluster parameters as described by the formulae above and \n",
    "        store them in their respective class variables\n",
    "        Params : \n",
    "            posterior_matrix : (np.array) matrix of size N X K containing posterior probability values\n",
    "        RETURN :\n",
    "            None\n",
    "        '''\n",
    "        pass\n",
    "            \n",
    "    def exp_maximize(self, epochs):\n",
    "        '''\n",
    "        Peform the expectation-maximization method as dicussed above by calling the functions in their \n",
    "        respective sequence. Also plot the progress of the process by calling the plot_progress function\n",
    "        after every regular interval of epochs.\n",
    "        Params : \n",
    "            epochs : (integer) maximum number of epochs to run the loop for\n",
    "        RETURN :\n",
    "            None\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def plot_progress(self):\n",
    "        points = np.linspace(np.min(self.X),np.max(self.X),500)\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.xlabel(\"$x$\")\n",
    "        plt.ylabel(\"pdf\")\n",
    "        plt.plot(self.X, 0.1*np.ones_like(self.X), 'x', color='navy')\n",
    "        for k in range(self.K):\n",
    "            plt.plot(points, [self.compute_pdf(p, k) for p in points])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EEEE\n",
      "======================================================================\n",
      "ERROR: test_compute_pdf (__main__.GMMTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-c6e34d736d95>\", line 5, in setUp\n",
      "    self.X = np.array([0.1, 1.2, 0.3, 0.4, 0.3, 3.5, 2.9])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_compute_pdf_matrix (__main__.GMMTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-c6e34d736d95>\", line 5, in setUp\n",
      "    self.X = np.array([0.1, 1.2, 0.3, 0.4, 0.3, 3.5, 2.9])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_reestimate_params (__main__.GMMTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-c6e34d736d95>\", line 5, in setUp\n",
      "    self.X = np.array([0.1, 1.2, 0.3, 0.4, 0.3, 3.5, 2.9])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: tests_compute_posterior (__main__.GMMTester)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-c6e34d736d95>\", line 5, in setUp\n",
      "    self.X = np.array([0.1, 1.2, 0.3, 0.4, 0.3, 3.5, 2.9])\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "FAILED (errors=4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=4 failures=0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class GMMTester(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.X = np.array([0.1, 1.2, 0.3, 0.4, 0.3, 3.5, 2.9])\n",
    "        self.means = [-2.0, 2.5]\n",
    "        self.variances = [1.0, 1.3]\n",
    "        self.weights = [0.1, 0.2]\n",
    "        \n",
    "        self.cluster_obj_1 = GaussianMixtureModel1D(self.X, 2)\n",
    "        self.cluster_obj_1.mean = self.means\n",
    "        self.cluster_obj_1.variance = self.variances\n",
    "        self.cluster_obj_1.weight = self.weights\n",
    "        \n",
    "        self.cluster_obj_2 = GaussianMixtureModel1D(self.X, 2)\n",
    "        self.cluster_obj_2.mean = self.means\n",
    "        self.cluster_obj_2.variance = self.variances\n",
    "        self.cluster_obj_2.weight = self.weights\n",
    "        \n",
    "        self.cluster_obj_3 = GaussianMixtureModel1D(self.X, 2)\n",
    "        self.cluster_obj_3.mean = self.means\n",
    "        self.cluster_obj_3.variance = self.variances\n",
    "        self.cluster_obj_3.weight = self.weights\n",
    "        \n",
    "        self.cluster_obj_4 = GaussianMixtureModel1D(self.X, 2)\n",
    "        self.cluster_obj_4.mean = self.means\n",
    "        self.cluster_obj_4.variance = self.variances\n",
    "        self.cluster_obj_4.weight = self.weights\n",
    "        \n",
    "    def test_compute_pdf(self):\n",
    "        \"\"\"\n",
    "        Test compute_pdf function from GaussianMixtureModel1D\n",
    "        \"\"\"\n",
    "        pdf = self.cluster_obj_1.compute_pdf(self.X[0], 1)\n",
    "        self.assertEqual(round(pdf, 3), 0.038)\n",
    "        \n",
    "    def test_compute_pdf_matrix(self):\n",
    "        \"\"\"\n",
    "        Test compute_pdf_matrix function from GaussianMixtureModel1D\n",
    "        \"\"\"\n",
    "        pdf_matrix = self.cluster_obj_2.compute_pdf_matrix()\n",
    "        self.assertEqual(round(pdf_matrix[0,0], 3), 0.044)\n",
    "        self.assertEqual(round(pdf_matrix[0,1], 3), 0.038)\n",
    "        \n",
    "    def tests_compute_posterior(self):\n",
    "        \"\"\"\n",
    "        Test compute_posterior function from GaussianMixtureModel1D\n",
    "        \"\"\"\n",
    "        pdf_matrix = self.cluster_obj_3.compute_pdf_matrix()\n",
    "        posterior_matrix = self.cluster_obj_3.compute_posterior(pdf_matrix)\n",
    "        self.assertEqual(round(posterior_matrix[0,0], 2), 0.37)\n",
    "        self.assertEqual(round(posterior_matrix[0,1], 2), 0.63)\n",
    "        \n",
    "    def test_reestimate_params(self):\n",
    "        \"\"\"\n",
    "        Test reestimate_params function from GaussianMixtureModel1D\n",
    "        \"\"\"\n",
    "        pdf_matrix = self.cluster_obj_4.compute_pdf_matrix()\n",
    "        posterior_matrix = self.cluster_obj_4.compute_posterior(pdf_matrix)\n",
    "        self.cluster_obj_4.reestimate_params(posterior_matrix)\n",
    "        self.assertEqual(round(self.cluster_obj_4.mean[0], 2), 0.24)\n",
    "        self.assertEqual(round(self.cluster_obj_4.variance[0], 2), 0.02)\n",
    "        self.assertEqual(round(self.cluster_obj_4.weight[0], 2), 0.13)\n",
    "        \n",
    "    \n",
    "tests = GMMTester()\n",
    "myTests = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(myTests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [5 pts] Using the Gaussian Mixture Model you have built, fit a model for the 1-D data in `Data1D` with $K=3$. Train the model for 20 epochs and plot the progress of the model using the function `plot_progress` in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
